{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6158fdd-a16d-4187-a8e6-d85a14f62910",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import ttest_rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16ad55e-e50b-4cc5-ba58-b50d73b598af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spikeinterface.full as si\n",
    "import spikeinterface.extractors as se\n",
    "import spikeinterface.widgets as sw\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "from spikeinterface import NumpyRecording, NumpySorting\n",
    "from spikeinterface import append_recordings, concatenate_recordings\n",
    "\n",
    "# %matplotlib widget\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f41b98-7b92-4c96-9eac-4b45d2c6a936",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = Path('/Users/shiqi/Desktop/spikeinterface/POD/o1_part6')\n",
    "# base_folder = Path(\"/home/alessio/Documents/data/spiketutorials/Official_Tutorial_SI_0.99_Nov23/\")\n",
    "oe_folder1 = base_folder / 'before'\n",
    "oe_folder2 = base_folder / 'after'\n",
    "full_raw_rec1 = si.read_openephys(oe_folder1)\n",
    "fs = full_raw_rec1.get_sampling_frequency()\n",
    "full_raw_rec1_sub = full_raw_rec1.frame_slice(start_frame=0*fs, end_frame=10*fs)\n",
    "full_raw_rec2 = si.read_openephys(oe_folder2)\n",
    "full_raw_rec2_sub = full_raw_rec2.frame_slice(start_frame=0*fs, end_frame=10*fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ad5a41-284e-4c69-87fe-56e06bbabfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = full_raw_rec1.get_sampling_frequency()\n",
    "full_raw_rec = si.concatenate_recordings([full_raw_rec1_sub, full_raw_rec2_sub])\n",
    "#full_raw_rec = full_raw_rec1_sub\n",
    "#full_raw_rec.get_probe().to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4daed7b7-f89e-48f1-9cfa-0aa4df60c9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.signal as signal\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "rec1 = si.bandpass_filter(full_raw_rec, freq_min=300, freq_max=6000)\n",
    "bad_channel_ids, channel_labels = si.detect_bad_channels(rec1)\n",
    "rec2 = rec1.remove_channels(bad_channel_ids)\n",
    "print('bad_channel_ids', bad_channel_ids)\n",
    "\n",
    "rec3 = si.phase_shift(rec2)\n",
    "rec4 = si.common_reference(rec3, operator=\"median\", reference=\"global\")\n",
    "rec = rec4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ddd0e7-1933-4456-819b-c3987812cb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sorting_SC = se.read_spykingcircus(folder_path=\"/Users/shiqi/Desktop/spikeinterface/POD/y_part7/folder_SC_y_p7/sorter_output\")\n",
    "job_kwargs = dict(n_jobs=40, chunk_duration='1s', progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bc5273-1bc3-4cc6-a5f8-566fcfd332a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spikeinterface.sortingcomponents.peak_detection import detect_peaks\n",
    "from spikeinterface.sortingcomponents.peak_localization import localize_peaks\n",
    "peaks = detect_peaks(rec,  method='locally_exclusive', detect_threshold=6, radius_um=50., **job_kwargs)\n",
    "peak_locations = localize_peaks(rec, peaks, method='center_of_mass', radius_um=50., **job_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c7f4c6-1f08-46fd-bc0f-dc03c1c828e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spike_x_locations = peak_locations['x']\n",
    "spike_y_locations = peak_locations['y']\n",
    "spike_times = peaks['sample_index'] / fs\n",
    "time_window = 300  # seconds\n",
    "valid_time_indices = spike_times <= time_window\n",
    "spike_times = spike_times[valid_time_indices]\n",
    "spike_x_locations = spike_x_locations[valid_time_indices]\n",
    "spike_y_locations = spike_y_locations[valid_time_indices]\n",
    "\n",
    "# Initialize plot with 4 horizontal subplots\n",
    "fig, axes = plt.subplots(1, 4, figsize=(15, 5))\n",
    "\n",
    "# Define the number of shanks\n",
    "num_shanks = 4\n",
    "\n",
    "# Define the boundaries for each shank based on x-coordinates\n",
    "shank_boundaries = np.linspace(np.min(spike_x_locations), np.max(spike_x_locations), num_shanks + 1)\n",
    "\n",
    "# Define colors for each shank\n",
    "colors = [(1, 0.5, 0.5), (1, 0.5, 0), (0, 0.75, 0.5), (0.5, 0.5, 1)] \n",
    "\n",
    "# Plot each spike, assigning them to shanks based on x-coordinates\n",
    "for shank_id in range(num_shanks):\n",
    "    shank_mask = (spike_x_locations >= shank_boundaries[shank_id]) & (spike_x_locations < shank_boundaries[shank_id + 1])\n",
    "    shank_spike_times = spike_times[shank_mask]\n",
    "    shank_spike_y_locations = spike_y_locations[shank_mask]\n",
    "\n",
    "    axes[shank_id].scatter(shank_spike_times, shank_spike_y_locations, s=1, color=colors[shank_id])\n",
    "    #axes[shank_id].set_title(f'Shank {shank_id}')\n",
    "    #axes[shank_id].set_xlabel('Time (s)')\n",
    "    #axes[shank_id].set_xticks([])\n",
    "    #axes[shank_id].set_yticks([])\n",
    "\n",
    "    # Remove spines (boundaries)\n",
    "    axes[shank_id].spines['top'].set_visible(False)\n",
    "    axes[shank_id].spines['right'].set_visible(False)\n",
    "    axes[shank_id].spines['left'].set_visible(False)\n",
    "    axes[shank_id].spines['bottom'].set_visible(False)\n",
    "    \n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.savefig('spike_plot.png', format='png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c514d5f-d804-4f98-8cf4-dda251a62466",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sorting_SC = se.SpykingCircusSortingExtractor(base_folder / 'folder_POD/sorter_output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d48403-1f50-4048-837c-a2c229ed5323",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spikeinterface.sorters import run_sorter\n",
    "sorting_SC = run_sorter(sorter_name=\"spykingcircus\", recording=rec, detect_sign= -1,\n",
    "                        detect_threshold=6, output_folder=base_folder / 'folder_part6', filter = False, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84da1c39-0413-4b3a-9d10-997723b7cd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorting_SC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb50f05-e55f-469d-b7db-1a6b172015f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorting_analyzer = si.create_sorting_analyzer(sorting_SC, rec,\n",
    "                                              format=\"binary_folder\", folder=base_folder / 'my_sorting_analyzer_part6',\n",
    "                                              **job_kwargs)\n",
    "sorting_analyzer.compute(\"random_spikes\", method=\"uniform\", max_spikes_per_unit=500)\n",
    "waveform=sorting_analyzer.compute(\"waveforms\", **job_kwargs)\n",
    "templates=sorting_analyzer.compute(\"templates\", **job_kwargs)\n",
    "noise=sorting_analyzer.compute(\"noise_levels\")\n",
    "unit_loc=sorting_analyzer.compute(\"unit_locations\", method=\"monopolar_triangulation\")\n",
    "spike_loc = sorting_analyzer.compute(input=\"spike_locations\",method=\"center_of_mass\")\n",
    "isi=sorting_analyzer.compute(\"isi_histograms\")\n",
    "corr=sorting_analyzer.compute(\"correlograms\", window_ms=100, bin_ms=0.5)\n",
    "#pc=sorting_analyzer.compute(\"principal_components\", n_components=3, mode='by_channel_global', whiten=True, **job_kwargs)\n",
    "#quality_metrics=sorting_analyzer.compute(\"quality_metrics\", metric_names=[\"snr\", \"firing_rate\"])\n",
    "similarity=sorting_analyzer.compute(\"template_similarity\")\n",
    "amplitute=sorting_analyzer.compute(\"spike_amplitudes\", **job_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7106ac79-6ccd-4706-ab6d-3f0c37a96c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "traces = rec.get_traces()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc208ff-7c38-4af0-8c1c-bbb7b101ca6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "start_sample = 12000\n",
    "channel_range = range(130, 140)  # Channels 10 through 21 (inclusive)\n",
    "time_window = 2  # seconds\n",
    "num_samples = int(fs * time_window)\n",
    "end_sample = start_sample + num_samples\n",
    "\n",
    "selected_traces = traces[start_sample:end_sample, channel_range]\n",
    "\n",
    "time_axis = np.arange(start_sample, end_sample) / fs\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "cmap = plt.get_cmap('cool', len(channel_range))\n",
    "\n",
    "for i, channel in enumerate(channel_range):\n",
    "    plt.plot(time_axis, selected_traces[:, i] + i * 800, color=cmap(i), label=f'Channel {channel + 1}')  # Offset each trace for clarity\n",
    "\n",
    "plt.xlabel('Time (s)')\n",
    "plt.yticks([])\n",
    "\n",
    "plt.savefig('raw_traces_channels_10_to_21_10s_gradient.png', format='png', dpi=300)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca041506-8f7e-40a2-a4dd-9581083c7d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_traces.shape\n",
    "selected_traces=np.array(selected_traces, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b66d47-cd5d-4d9a-8954-b8dd97d5f050",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import find_peaks\n",
    "num_channels = selected_traces.shape[1]\n",
    "all_peaks = {}  # Dictionary to store peaks for each channel\n",
    "\n",
    "for i in range(num_channels):\n",
    "    # Calculate the standard deviation of the trace\n",
    "    std_dev = np.std(selected_traces[:, i])\n",
    "    \n",
    "    # Set the negative threshold as -5 * std\n",
    "    threshold = -3 * std_dev\n",
    "    \n",
    "    # Find all peaks below the negative threshold\n",
    "    peaks, properties = find_peaks(-selected_traces[:, i], height=-threshold)\n",
    "    \n",
    "    # Store the peaks and their properties\n",
    "    all_peaks[f\"Channel_{i + channel_range.start}\"] = {\n",
    "        'peaks': peaks,\n",
    "        'peak_heights': properties['peak_heights']\n",
    "    }\n",
    "\n",
    "# Print or use the peak information\n",
    "for channel, peak_info in all_peaks.items():\n",
    "    print(f\"{channel}: Found {len(peak_info['peaks'])} peaks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b7bc81-ba81-4820-b5b0-6603fea3906d",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_points = {}\n",
    "for channel, peak_info in all_peaks.items():\n",
    "    # Calculate the time for each peak index\n",
    "    time_points[channel] = peak_info['peaks'] / fs\n",
    "\n",
    "# Create a scatter plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot each channel's peaks as a scatter plot\n",
    "for i, (channel, times) in enumerate(time_points.items()):\n",
    "    plt.scatter(times, [i+channel_range.start] * len(times), label=channel)\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Channel')\n",
    "plt.title('Scatter Plot of Peak Time Points')\n",
    "plt.yticks(range(channel_range.start, channel_range.stop))  # Set y-axis ticks to the channel numbers\n",
    "plt.legend(title=\"Channels\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5952a7e6-c775-47ea-ba44-93269483abcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = 30000  # Define your sampling frequency\n",
    "num_samples = selected_traces.shape[0]\n",
    "time_axis = np.arange(start_sample, end_sample) / fs  # Create the time axis based on start_sample and fs\n",
    "\n",
    "# Create the figure with desired size\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Get the colormap\n",
    "cmap = plt.get_cmap('cool', len(channel_range))\n",
    "\n",
    "# Plot the selected traces with offset for clarity\n",
    "for i, channel in enumerate(channel_range):\n",
    "    plt.plot(time_axis, selected_traces[:, i] + i * 800, color=cmap(i), label=f'Channel {channel}')  # Offset each trace\n",
    "    \n",
    "    # Add scatter plot of peak points\n",
    "    if f\"Channel_{channel}\" in all_peaks:\n",
    "        peak_indices = all_peaks[f\"Channel_{channel}\"]['peaks']\n",
    "        peak_times = peak_indices / fs + start_sample / fs  # Convert to time points\n",
    "        peak_amplitudes = selected_traces[peak_indices, i] + i * 800  # Use offset to match trace\n",
    "        plt.scatter(peak_times, peak_amplitudes, color=cmap(i), edgecolor='k', zorder=3)\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude + Offset')\n",
    "plt.title('Traces with Peak Detection')\n",
    "plt.yticks([])  # Remove y-ticks for clarity\n",
    "plt.legend(title=\"Channels\", bbox_to_anchor=(1.05, 1), loc='upper left')  # Place legend outside\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig('raw_traces_with_peaks.png', format='png', dpi=300)\n",
    "\n",
    "# Show the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8350d0-9d22-458b-b771-a0565234b776",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_window_ms = 3  # 3 ms window\n",
    "window_samples = int(fs * (time_window_ms / 1000))  # Number of samples for 3 ms window\n",
    "half_window = window_samples // 2  # Half window for centering\n",
    "\n",
    "# Dictionary to store extracted waveforms for each channel\n",
    "extracted_spikes = {}\n",
    "\n",
    "# Iterate through each channel to extract spikes\n",
    "for channel in channel_range:\n",
    "    channel_label = f\"Channel_{channel}\"\n",
    "    if channel_label in all_peaks:\n",
    "        peak_indices = all_peaks[channel_label]['peaks']\n",
    "        \n",
    "        # List to store waveforms for the current channel\n",
    "        channel_waveforms = []\n",
    "        \n",
    "        # Extract waveforms around each peak\n",
    "        for peak in peak_indices:\n",
    "            start_idx = peak - half_window\n",
    "            end_idx = peak + half_window\n",
    "            \n",
    "            # Ensure indices are within the bounds of the trace\n",
    "            if start_idx >= 0 and end_idx < traces.shape[0]:\n",
    "                # Extract the waveform for this peak\n",
    "                waveform = traces[start_idx:end_idx, channel - channel_range.start]\n",
    "                channel_waveforms.append(waveform)\n",
    "        \n",
    "        # Store the waveforms in the dictionary for the current channel\n",
    "        extracted_spikes[channel_label] = {\n",
    "            'waveforms': np.array(channel_waveforms)\n",
    "        }\n",
    "\n",
    "# Check the extracted spikes for each channel\n",
    "for channel, data in extracted_spikes.items():\n",
    "    print(f\"{channel}: {len(data['waveforms'])} spikes extracted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab21b66-dff1-4b1f-9eb8-92f428f96736",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_window_ms = 3  # 5 ms time window\n",
    "window_samples = int(fs * (time_window_ms / 1000))  # Number of samples for 5 ms window\n",
    "half_window = window_samples // 2  # Half window for centering\n",
    "\n",
    "# Dictionary to store 5 ms extracted waveforms for each channel\n",
    "extracted_waveforms = {}\n",
    "\n",
    "# Iterate through each channel and extract waveforms around peaks\n",
    "for channel in channel_range:\n",
    "    channel_label = f\"Channel_{channel}\"\n",
    "    if channel_label in all_peaks:\n",
    "        peak_indices = all_peaks[channel_label]['peaks']\n",
    "        \n",
    "        # List to store waveforms for the current channel\n",
    "        waveforms = []\n",
    "        \n",
    "        # Extract waveforms around each peak\n",
    "        for peak in peak_indices:\n",
    "            start_idx = peak - half_window\n",
    "            end_idx = peak + half_window\n",
    "            \n",
    "            # Ensure indices are within the bounds of the trace\n",
    "            if start_idx >= 0 and end_idx < selected_traces.shape[0]:\n",
    "                # Extract the waveform around this peak\n",
    "                waveform = selected_traces[start_idx:end_idx, channel - channel_range.start]\n",
    "                waveforms.append(waveform)\n",
    "        \n",
    "        # Store the waveforms in the dictionary for the current channel\n",
    "        extracted_waveforms[channel_label] = np.array(waveforms)\n",
    "\n",
    "# Check the extracted waveforms for each channel\n",
    "for channel, waveforms in extracted_waveforms.items():\n",
    "    print(f\"{channel}: {waveforms.shape[0]} spikes extracted, each with shape {waveforms.shape[1:]}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd6f66c-3ad0-42d0-b1e8-423272e4c8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_time_axis = np.linspace(-half_window, half_window, window_samples) / fs * 1000  # Convert to ms\n",
    "\n",
    "# Get the colormap for the channels\n",
    "cmap = plt.get_cmap('cool', len(channel_range))\n",
    "\n",
    "# Sort the extracted_waveforms dictionary by channel numbers and reverse the order\n",
    "sorted_channels = sorted(extracted_waveforms.keys(), key=lambda x: int(x.split('_')[1]), reverse=True)\n",
    "\n",
    "# Create a figure for plotting all flattened 5 ms waveforms sequentially on x-axis for each channel\n",
    "fig, axes = plt.subplots(len(sorted_channels), 1, figsize=(15, len(sorted_channels) * 3), sharey=True)\n",
    "\n",
    "# If only one channel, convert axes to a list for consistency\n",
    "if len(sorted_channels) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "# Iterate through each channel in reversed order and plot each flattened 5 ms waveform sequentially on the x-axis\n",
    "for i, channel_label in enumerate(sorted_channels):\n",
    "    waveforms = extracted_waveforms[channel_label]\n",
    "    \n",
    "    # Set initial x offset\n",
    "    x_offset = 0\n",
    "    \n",
    "    # Get the color for the current channel from the colormap\n",
    "    channel_index = int(channel_label.split('_')[1]) - min(channel_range)  # Adjust for color mapping\n",
    "    channel_color = cmap(channel_index)\n",
    "    \n",
    "    # Plot each flattened waveform with a horizontal offset and the channel color\n",
    "    for waveform in waveforms:\n",
    "        # Flatten the waveform to 1D\n",
    "        flattened_waveform = waveform.flatten()\n",
    "        \n",
    "        # Create a new time axis for this flattened waveform with the offset\n",
    "        time_axis = np.arange(len(flattened_waveform)) + x_offset\n",
    "        \n",
    "        # Plot the flattened waveform on the current channel's subplot with the assigned color\n",
    "        axes[i].plot(time_axis, flattened_waveform, alpha=0.8, color=channel_color)\n",
    "        \n",
    "        # Update x_offset for the next waveform with a gap\n",
    "        x_offset += len(flattened_waveform) + 10  # Add a gap between waveforms (adjust gap as needed)\n",
    "    \n",
    "    # Remove grid, axis labels, and ticks for this subplot\n",
    "    axes[i].grid(False)  # Remove grid\n",
    "    axes[i].set_xticks([])  # Remove x-axis ticks\n",
    "    axes[i].set_yticks([])  # Remove y-axis ticks\n",
    "    axes[i].set_frame_on(False)  # Remove the box frame around the subplot\n",
    "\n",
    "# Remove common x-label and set a minimal title\n",
    "plt.suptitle('Flattened 5 ms Window Spikes for Each Channel (Sequential on X-axis)', fontsize=16)\n",
    "plt.subplots_adjust(top=0.9, bottom=0.1, left=0.05, right=0.95, hspace=0.4)  # Adjust layout\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758d11ea-29fd-4450-be74-6c02c5df047a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Flatten and concatenate all waveforms\n",
    "all_waveforms = []\n",
    "channel_labels = []\n",
    "\n",
    "for channel_label, waveforms in extracted_waveforms.items():\n",
    "    # Flatten each waveform and add to list\n",
    "    flattened_waveforms = [waveform.flatten() for waveform in waveforms]\n",
    "    all_waveforms.extend(flattened_waveforms)\n",
    "    channel_labels.extend([channel_label] * len(flattened_waveforms))  # Keep track of channel for each waveform\n",
    "\n",
    "# Convert to numpy array\n",
    "all_waveforms = np.array(all_waveforms)\n",
    "\n",
    "# Check the shape of the concatenated waveforms\n",
    "print(f\"Total waveforms shape: {all_waveforms.shape}\")\n",
    "\n",
    "# Step 2: Apply PCA\n",
    "n_components = 3  # Number of components for PCA\n",
    "pca = PCA(n_components=n_components)\n",
    "pca_result = pca.fit_transform(all_waveforms)\n",
    "\n",
    "# Explained variance by each component\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "print(f\"Explained variance ratio: {explained_variance}\")\n",
    "\n",
    "# Step 3: Spike Sorting Using K-means Clustering\n",
    "n_clusters = 4  # You can change this based on expected neuron types\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "cluster_labels = kmeans.fit_predict(pca_result)\n",
    "\n",
    "# Step 4: Visualization of PCA Clustering Results\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "\n",
    "# Scatter plot of the first two principal components, colored by cluster labels\n",
    "scatter = ax.scatter(pca_result[:, 0], pca_result[:, 1], c=cluster_labels, cmap='viridis', alpha=0.8, edgecolor='k')\n",
    "\n",
    "# Add a colorbar and labels\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('PCA of Extracted Waveforms with K-means Clustering')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Step 5: Plotting the average waveforms of each cluster\n",
    "fig, axs = plt.subplots(n_clusters, 1, figsize=(10, n_clusters * 3))\n",
    "\n",
    "for cluster in range(n_clusters):\n",
    "    # Extract waveforms belonging to the current cluster\n",
    "    cluster_waveforms = all_waveforms[cluster_labels == cluster]\n",
    "    \n",
    "    # Calculate the average waveform\n",
    "    average_waveform = np.mean(cluster_waveforms, axis=0)\n",
    "    \n",
    "    # Plot all waveforms and the average waveform\n",
    "    for waveform in cluster_waveforms:\n",
    "        axs[cluster].plot(waveform, color='gray', alpha=0.3)\n",
    "    axs[cluster].plot(average_waveform, color='red', linewidth=2, label=f'Cluster {cluster + 1} Average')\n",
    "    \n",
    "    # Set labels and title\n",
    "    axs[cluster].set_title(f'Cluster {cluster + 1} Waveforms')\n",
    "    axs[cluster].set_xlabel('Sample Index')\n",
    "    axs[cluster].set_ylabel('Amplitude')\n",
    "    axs[cluster].legend()\n",
    "    axs[cluster].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ee0b64-b41a-44eb-9ac2-2d1b8c170941",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "noise_clusters = [1, 2]  # Replace with the actual noise cluster numbers\n",
    "\n",
    "filtered_waveforms = all_waveforms[~np.isin(cluster_labels, noise_clusters)]\n",
    "filtered_pca_result = pca_result[~np.isin(cluster_labels, noise_clusters)]\n",
    "filtered_cluster_labels = cluster_labels[~np.isin(cluster_labels, noise_clusters)]\n",
    "\n",
    "# Check the new shape of the filtered data\n",
    "print(f\"Filtered waveforms shape: {filtered_waveforms.shape}\")\n",
    "print(f\"Filtered PCA result shape: {filtered_pca_result.shape}\")\n",
    "\n",
    "# Step 3: Define custom light blue and light purple colors for clusters\n",
    "# Ensure that the number of colors matches the number of clusters (excluding noise)\n",
    "unique_clusters = np.unique(filtered_cluster_labels)\n",
    "cluster_colors = {unique_clusters[0]: '#1E90FF', unique_clusters[1]: '#9370DB'}  # Light blue and light purple\n",
    "\n",
    "# Map the cluster labels to colors for visualization\n",
    "cluster_colors_array = [cluster_colors[cluster] for cluster in filtered_cluster_labels]\n",
    "\n",
    "# Visualize the PCA without Noise Clusters using the custom light blue and light purple colors\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "\n",
    "# Scatter plot of the first two principal components, excluding the noise clusters\n",
    "scatter = ax.scatter(filtered_pca_result[:, 0], filtered_pca_result[:, 1], \n",
    "                     c=cluster_colors_array, alpha=0.8, edgecolor='k')\n",
    "\n",
    "# Add labels\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('PCA of Extracted Waveforms without Noise Clusters (Light Blue and Light Purple)')\n",
    "ax.grid(False)  # Remove grid\n",
    "plt.show()\n",
    "\n",
    "# Step 4: Plotting the Average Waveforms of Each Non-Noise Cluster with Light Blue and Light Purple\n",
    "fig, axs = plt.subplots(len(unique_clusters), 1, figsize=(6, len(unique_clusters) * 3))\n",
    "\n",
    "for i, cluster in enumerate(unique_clusters):\n",
    "    # Extract waveforms belonging to the current cluster\n",
    "    cluster_waveforms = filtered_waveforms[filtered_cluster_labels == cluster]\n",
    "    \n",
    "    # Calculate the average waveform\n",
    "    average_waveform = np.mean(cluster_waveforms, axis=0)\n",
    "    \n",
    "    # Plot all waveforms in the cluster color and the average waveform in the same color\n",
    "    for waveform in cluster_waveforms:\n",
    "        axs[i].plot(waveform, color=cluster_colors[cluster], alpha=0.3)\n",
    "    axs[i].plot(average_waveform, color=cluster_colors[cluster], linewidth=2, label=f'Cluster {cluster + 1} Average')\n",
    "    \n",
    "    axs[i].grid(False)  # Remove grid\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920d84b8-27b3-441c-9f5a-cb9d851061af",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_metrics = si.compute_template_metrics(sorting_analyzer)\n",
    "display(template_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34370244-9da3-4a47-a70f-81d02a0012e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr=np.load(base_folder/'my_sorting_analyzer_part7_2/extensions/correlograms/ccgs.npy')\n",
    "bins=np.load(base_folder/'my_sorting_analyzer_part7_2/extensions/correlograms/bins.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8daabc7-f83b-4346-bb0b-c7950296c06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "def fit_ACG(acg_narrow, plots=True):\n",
    "    # Setting up the time bins and parameters\n",
    "    offset = 100\n",
    "    x = np.arange(1, 101) / 2.0  # Time bins in milliseconds\n",
    "    \n",
    "    # Setting time-zero bin to zero (-0.5ms -> 0.5ms)\n",
    "    acg_narrow[99:101] = 0\n",
    "    \n",
    "    # Variables for initial parameters and bounds for curve fitting\n",
    "    a0 = [20, 1, 30, 2, 0.5, 5, 1.5, 2]\n",
    "    lb = [1, 0.1, 0, 0, -30, 0, 0.1, 0]\n",
    "    ub = [500, 50, 500, 15, 50, 20, 5, 100]\n",
    "    \n",
    "    fit_params = np.nan * np.ones((8,))\n",
    "    rsquare = np.nan\n",
    "    \n",
    "    # Define the fitting equation\n",
    "    def predicted(x, a, b, c, d, e, f, g, h):\n",
    "        return np.maximum(c * (np.exp(-(x - f) / a) - d * np.exp(-(x - f) / b)) + h, 0)\n",
    "    \n",
    "    # Perform the fitting\n",
    "    try:\n",
    "        popt, _ = curve_fit(predicted, x, acg_narrow, p0=a0, bounds=(lb, ub))\n",
    "        \n",
    "        # Generate fitted curve\n",
    "        fitted_curve = predicted(x, *popt)\n",
    "        \n",
    "        # Calculate R-squared value\n",
    "        residuals = acg_narrow - fitted_curve\n",
    "        ss_res = np.sum(residuals**2)\n",
    "        ss_tot = np.sum((acg_narrow - np.mean(acg_narrow))**2)\n",
    "        rsquare = 1 - (ss_res / ss_tot)\n",
    "        \n",
    "        # Plotting\n",
    "        if plots:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(x, acg_narrow, label='Auto-correlogram Data', color='b')\n",
    "            plt.plot(x, fitted_curve, 'r-', label=f'Rise={popt[1]:.3f}, Decay={popt[0]:.3f}')\n",
    "            plt.title('Exponential Fit of Auto-Correlogram')\n",
    "            plt.xlabel('Time Bins (ms)')\n",
    "            plt.ylabel('Spike Count')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        # Return fit parameters and R-squared value\n",
    "        fit_params = {\n",
    "            'acg_tau_decay': popt[0],\n",
    "            'acg_tau_rise': popt[1],\n",
    "            'acg_c': popt[2],\n",
    "            'acg_d': popt[3],\n",
    "            'acg_asymptote': popt[4],\n",
    "            'acg_refrac': popt[5],\n",
    "            'acg_tau_burst': popt[6],\n",
    "            'acg_h': popt[7],\n",
    "            'acg_fit_rsquare': rsquare\n",
    "        }\n",
    "        \n",
    "    except RuntimeError:\n",
    "        print(\"Fit failed. Returning NaN values.\")\n",
    "    \n",
    "    return fit_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36de1716-01de-41f3-a41b-1c93140a68f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "acg_length = 100\n",
    "sample_acg = arr[54,54,100:200]\n",
    "\n",
    "# Fit exponential curve and plot results\n",
    "fit_params = fit_ACG(sample_acg)\n",
    "print(f\"Fit parameters: {fit_params}\")\n",
    "\n",
    "num_units = arr.shape[0]\n",
    "fit_params_units = np.empty((num_units, num_units), dtype=object)\n",
    "\n",
    "for i in range(num_units):\n",
    "    acg_unit = arr[i, i, 100:200]  # Extract the autocorrelogram for unit i\n",
    "    \n",
    "    fit_params = fit_ACG(acg_unit, plots=False)\n",
    "    \n",
    "    # Store the fit parameters\n",
    "    fit_params_units[i, i] = fit_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6574de-0d5c-465f-a814-66cda6eda51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_units = fit_params_units.shape[0]\n",
    "units_with_acg_tau_rise_gt_6 = []\n",
    "tau_rise_values = []\n",
    "for i in range(num_units):\n",
    "    fit_params = fit_params_units[i, i]  # Access the dictionary in the first column of each row\n",
    "    if fit_params is not None and len(fit_params_units[i, i]) == 9:\n",
    "        tau_rise_values.append(fit_params['acg_tau_rise'])\n",
    "        if fit_params['acg_tau_rise'] > 6:\n",
    "            units_with_acg_tau_rise_gt_6.append(i)\n",
    "\n",
    "# Get the unit IDs from sorting_SC\n",
    "unit_ids = sorting_SC.get_unit_ids()\n",
    "\n",
    "# Ensure the mapping is by the order of unit_ids\n",
    "#mapped_tau_rise = {unit_ids[i]: tau_rise_values[i] for i in range(len(unit_ids))}\n",
    "\n",
    "# Debugging: Print to verify the mapped tau_rise values\n",
    "#print(\"Mapped tau_rise by order:\", mapped_tau_rise)\n",
    "\n",
    "result_WI = [unit_ids[i] in [unit_ids[j] for j in units_with_acg_tau_rise_gt_6] for i in range(len(unit_ids))]\n",
    "\n",
    "#unit_ids = sorting_SC.get_unit_ids()\n",
    "#selected_unit_ids = [unit_ids[i] for i in units_with_acg_tau_rise_gt_6]\n",
    "#result_WI = [unit_id in selected_unit_ids for unit_id in unit_ids]\n",
    "\n",
    "len(units_with_acg_tau_rise_gt_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b1e0fe-f125-4d3f-a46e-1d8ab6a1b93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_units = []\n",
    "tau_rise_values = []\n",
    "\n",
    "# Filter out units with NaN in fit_params_units\n",
    "for i in range(num_units):\n",
    "    fit_params = fit_params_units[i, i]\n",
    "    if fit_params is not None and len(fit_params) == 9 and not np.isnan(fit_params['acg_tau_rise']):\n",
    "        valid_units.append(i)\n",
    "        tau_rise_values.append(fit_params['acg_tau_rise'])\n",
    "\n",
    "# Get the unit IDs from sorting_SC\n",
    "unit_ids = sorting_SC.get_unit_ids()\n",
    "\n",
    "# Map valid tau_rise_values to corresponding unit_ids\n",
    "# We filter unit_ids to match only valid units\n",
    "filtered_unit_ids = [unit_ids[i] for i in valid_units]\n",
    "mapped_tau_rise = {filtered_unit_ids[i]: tau_rise_values[i] for i in range(len(filtered_unit_ids))}\n",
    "result_WI = [unit_ids[i] in [unit_ids[j] for j in units_with_acg_tau_rise_gt_6] for i in range(len(unit_ids))]\n",
    "\n",
    "len(units_with_acg_tau_rise_gt_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dca6e0a-4a73-4b95-964c-aed08f6dd2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_units = fit_params_units.shape[0]\n",
    "units_with_acg_tau_rise_lt_6 = []\n",
    "\n",
    "tau_rise_values = []\n",
    "for i in range(num_units):\n",
    "    fit_params = fit_params_units[i, i]  # Access the dictionary in the first column of each row\n",
    "    if fit_params is not None and len(fit_params_units[i, i]) == 9:\n",
    "        tau_rise_values.append(fit_params['acg_tau_rise'])\n",
    "        if fit_params['acg_tau_rise'] < 6:\n",
    "            units_with_acg_tau_rise_lt_6.append(i)\n",
    "            \n",
    "result_pyramidal = [unit_ids[i] in [unit_ids[j] for j in units_with_acg_tau_rise_lt_6] for i in range(len(unit_ids))]\n",
    "len(units_with_acg_tau_rise_lt_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d3127f-dbd9-4aa0-bc15-3a53b890419e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spikeinterface.extractors as se\n",
    "from spikeinterface.postprocessing import compute_principal_components\n",
    "from spikeinterface.qualitymetrics import (\n",
    "    compute_snrs,\n",
    "    compute_firing_rates,\n",
    "    compute_isi_violations,\n",
    "    calculate_pc_metrics,\n",
    "    compute_quality_metrics,\n",
    "    compute_quality_metrics,\n",
    "    compute_amplitude_medians\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cd1496-f581-49c6-ab2c-bec027fa830c",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics1 = compute_quality_metrics(sorting_analyzer, metric_names=[\"firing_rate\", \"snr\", \"amplitude_cutoff\", \"amplitude_median\"])\n",
    "print(metrics1)\n",
    "metrics1.to_csv(base_folder /\"metrics1.csv\", index=False)  # Replace with your desired filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98da565a-063c-45d6-9be9-20a5bad3d43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorting_analyzer.compute(\"principal_components\", n_components=3, mode=\"by_channel_global\", whiten=True)\n",
    "\n",
    "metrics2 = compute_quality_metrics(\n",
    "    sorting_analyzer,\n",
    "    metric_names=[\n",
    "        \"isolation_distance\",\n",
    "        \"d_prime\",\n",
    "    ],\n",
    ")\n",
    "metrics2.to_csv(base_folder /\"metrics2.csv\", index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12f7d75-7ec8-40d2-82a9-f9a4629a1aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_mask = (metrics1[\"snr\"] > 3)  & (metrics2[\"isolation_distance\"] > 50) & (template_metrics[\"peak_to_valley\"] < 0.00045) \n",
    "print(keep_mask)\n",
    "keep_unit_ids1 = keep_mask[keep_mask].index.values\n",
    "keep_unit_ids1 = [unit_id for unit_id in keep_unit_ids1]\n",
    "#keep_unit_ids1 = [8, 290, 81, 238, 299, 164, 239, 132, 379, 302, 383, 312, 303, 385, 384, 135, 345, 7, 393, 347, 138, 40, 349, 359, 421, 427, 37, 438, 15, 199, 451, 155, 192]\n",
    "spike_trains_narrow = {unit_id: sorting_SC.get_unit_spike_train(unit_id) for unit_id in keep_unit_ids1}\n",
    "print(keep_unit_ids1)\n",
    "analyzer_narrow = sorting_analyzer.select_units(keep_unit_ids1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdb9e63-9340-44ca-8486-6333d22ce554",
   "metadata": {},
   "outputs": [],
   "source": [
    "narrow_sorting = sorting_SC.select_units(keep_unit_ids1)\n",
    "print(narrow_sorting)\n",
    "#narrow_sorting.save(folder=base_folder/'narrow_sorting')\n",
    "\n",
    "print(f\"Number of units before curation: {len(sorting_SC.get_unit_ids())}\")\n",
    "print(f\"Narrow interneuron: {len(narrow_sorting.get_unit_ids())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136bab4f-7abf-4ac1-a75a-9bc6a627f858",
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_mask = (metrics1[\"snr\"] > 3)  & (metrics2[\"isolation_distance\"] > 50) & (template_metrics[\"peak_to_valley\"] > 0.00045)  &  result_pyramidal   \n",
    "print(keep_mask) \n",
    "keep_unit_ids2 = keep_mask[keep_mask].index.values\n",
    "keep_unit_ids2 = [unit_id for unit_id in keep_unit_ids2]\n",
    "#keep_unit_ids2 = [10, 304, 184, 363, 173, 71, 6, 417, 100, 130, 254, 101, 177, 305, 318, 186, 406, 179, 319, 242, 42, 80, 257, 18, 190, 65, 420, 82, 125, 31, 180, 244, 14, 360, 245, 368, 259, 67, 140, 194, 361, 26, 182, 313, 247, 116, 315, 183, 249, 261, 260, 70, 316, 392, 277, 337, 375, 265, 198, 145, 328, 103, 144, 20, 91, 434, 446, 433, 104, 200, 222, 395, 28, 285, 93, 326, 105, 223, 34, 396, 19, 45, 96, 439, 95, 106, 167, 154, 270, 151, 387, 224, 327, 340, 450, 397, 271, 169, 401, 297, 403, 331, 324, 226, 2, 107, 111, 30, 334, 333, 273, 404, 228, 408, 426, 53, 119, 343, 64, 3, 172, 346, 234, 300, 276, 57]\n",
    "spike_trains_pyramidal = {unit_id: sorting_SC.get_unit_spike_train(unit_id) for unit_id in keep_unit_ids2}\n",
    "print(keep_unit_ids2)\n",
    "analyzer_pyramidal = sorting_analyzer.select_units(keep_unit_ids2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3f9cde-ee56-49c3-b333-321533a2f004",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyramidal_sorting = sorting_SC.select_units(keep_unit_ids2)\n",
    "print(pyramidal_sorting)\n",
    "#pyramidal_sorting.save(folder=base_folder/'pyramidal_sorting')\n",
    "print(f\"Number of units before curation: {len(sorting_SC.get_unit_ids())}\")\n",
    "print(f\"pyramidal waveform: {len(pyramidal_sorting.get_unit_ids())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1a40cb-13d6-4f2a-b654-f8df0d1ab214",
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_mask = (metrics1[\"snr\"] > 3)  & (metrics2[\"isolation_distance\"] > 50) & (template_metrics[\"peak_to_valley\"] > 0.00045)  & result_WI \n",
    "print(keep_mask)\n",
    "keep_unit_ids3 = keep_mask[keep_mask].index.values\n",
    "keep_unit_ids3 = [unit_id for unit_id in keep_unit_ids3]\n",
    "#keep_unit_ids3 = [295, 382, 400, 306, 405, 230, 311, 142, 414, 149, 41, 68, 416, 72, 436, 166, 447, 442, 48, 211, 219, 246, 225, 49, 221, 220]\n",
    "spike_trains_WI = {unit_id: sorting_SC.get_unit_spike_train(unit_id) for unit_id in keep_unit_ids3}\n",
    "print(keep_unit_ids3)\n",
    "analyzer_WI = sorting_analyzer.select_units(keep_unit_ids3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e098468a-8079-4af1-8aac-547f5a0c1d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "WI_sorting = sorting_SC.select_units(keep_unit_ids3)\n",
    "print(WI_sorting)\n",
    "#WI_sorting.save(folder=base_folder/'WI_sorting')\n",
    "print(f\"Number of units before curation: {len(sorting_SC.get_unit_ids())}\")\n",
    "print(f\"WI waveform: {len(WI_sorting.get_unit_ids())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2ecabb-1621-425c-a271-16e8f57ad477",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_size = int(fs)\n",
    "num_samples = full_raw_rec.get_num_samples()\n",
    "num_bins = num_samples // bin_size\n",
    "unit_locations=np.load(base_folder/'my_sorting_analyzer_part7_2/extensions/unit_locations/unit_locations.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b19805-854c-45c9-9204-9d1627ec44e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "spike_counts = np.zeros((len(keep_unit_ids1), num_bins), dtype=int)\n",
    "unit_id_to_location = {unit_id: unit_locations[i] for i, unit_id in enumerate(keep_unit_ids1)}\n",
    "\n",
    "sorted_unit_ids = sorted(keep_unit_ids1, key=lambda unit_id: unit_id_to_location[unit_id][1])\n",
    "\n",
    "spike_counts1 = np.zeros((len(sorted_unit_ids), num_bins), dtype=int)\n",
    "for unit_idx, unit_id in enumerate(sorted_unit_ids):\n",
    "    spike_train = spike_trains_narrow[unit_id]\n",
    "    binned_spikes, _ = np.histogram(spike_train, bins=num_bins, range=(0, num_samples))\n",
    "    spike_counts1[unit_idx, :] = binned_spikes\n",
    "normalized_spike_counts1 = spike_counts1 / spike_counts1.max(axis=1, keepdims=True)\n",
    "spike_counts_CA1_df = pd.DataFrame(spike_counts1)\n",
    "spike_counts_CA1_df.insert(0, 'Unit_ID', sorted_unit_ids) \n",
    "spike_counts_CA1_df.to_csv(base_folder / \"spike_count_NI_2.csv\", index=False)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(spike_counts1, aspect='auto', cmap='viridis', interpolation='nearest')\n",
    "#plt.xlim(0,600)\n",
    "plt.colorbar(label='Spike Count')\n",
    "plt.xlabel('Time (bins)')\n",
    "plt.ylabel('Units')\n",
    "plt.tight_layout()\n",
    "#plt.savefig(base_folder/'spike_raster_NI.png', format='png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d569a27-ee63-46fc-a96d-bd9b0ff4f5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "spike_counts_first_300 = normalized_spike_counts1[:, :300]\n",
    "spike_counts_second_300 = normalized_spike_counts1[:, 300:600]\n",
    "\n",
    "# Plotting the heatmaps as subplots\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# First 300 bins\n",
    "plt.subplot(1, 2, 1)  # 1 row, 2 columns, first subplot\n",
    "plt.imshow(spike_counts_first_300, aspect='auto', cmap='viridis', interpolation='nearest')\n",
    "plt.colorbar(label='Spike Count')\n",
    "plt.title('First 300 Bins')\n",
    "plt.xlabel('Time (bins)')\n",
    "plt.ylabel('Units')\n",
    "\n",
    "# Second 300 bins\n",
    "plt.subplot(1, 2, 2)  # 1 row, 2 columns, second subplot\n",
    "plt.imshow(spike_counts_second_300, aspect='auto', cmap='viridis', interpolation='nearest')\n",
    "plt.colorbar(label='Spike Count')\n",
    "plt.title('Second 300 Bins')\n",
    "plt.xlabel('Time (bins)')\n",
    "plt.ylabel('Units')\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "plt.savefig(base_folder/'spike_raster_NI.pdf', format='pdf', dpi=600)\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f743b683-d6a0-4467-8b37-041b549ffd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "spike_counts2 = np.zeros((len(keep_unit_ids2), num_bins), dtype=int)\n",
    "unit_id_to_location = {unit_id: unit_locations[i] for i, unit_id in enumerate(keep_unit_ids2)}\n",
    "\n",
    "sorted_unit_ids = sorted(keep_unit_ids2, key=lambda unit_id: unit_id_to_location[unit_id][1])\n",
    "\n",
    "spike_counts = np.zeros((len(sorted_unit_ids), num_bins), dtype=int)\n",
    "differences = []\n",
    "for unit_idx, unit_id in enumerate(sorted_unit_ids):\n",
    "    spike_train = spike_trains_pyramidal[unit_id]\n",
    "    binned_spikes, _ = np.histogram(spike_train, bins=num_bins, range=(0, num_samples))\n",
    "    spike_counts2[unit_idx, :] = binned_spikes\n",
    "normalized_spike_counts2 = spike_counts2 / spike_counts2.max(axis=1, keepdims=True)\n",
    "spike_counts_CA1_df = pd.DataFrame(spike_counts2)\n",
    "spike_counts_CA1_df.insert(0, 'Unit_ID', sorted_unit_ids) \n",
    "spike_counts_CA1_df.to_csv(base_folder / \"spike_count_PC_2.csv\", index=False)\n",
    "\n",
    "plt.figure(figsize=(4, 6))\n",
    "plt.imshow(spike_counts2, aspect='auto', cmap='viridis', interpolation='nearest')\n",
    "#plt.xlim(0,600)\n",
    "plt.colorbar(label='Spike Count')\n",
    "plt.xlabel('Time (bins)')\n",
    "plt.ylabel('Units')\n",
    "plt.tight_layout()\n",
    "#plt.savefig(base_folder/'spike_raster_PC.png', format='png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcad99c-4420-4da0-bea1-6c85b36395d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spike_counts_first_300 = normalized_spike_counts2[:, :300]\n",
    "spike_counts_second_300 = normalized_spike_counts2[:, 300:600]\n",
    "\n",
    "# Plotting the heatmaps as subplots\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# First 300 bins\n",
    "plt.subplot(1, 2, 1)  # 1 row, 2 columns, first subplot\n",
    "plt.imshow(spike_counts_first_300, aspect='auto', cmap='viridis', interpolation='nearest')\n",
    "plt.colorbar(label='Spike Count')\n",
    "plt.title('First 300 Bins')\n",
    "plt.xlabel('Time (bins)')\n",
    "plt.ylabel('Units')\n",
    "\n",
    "# Second 300 bins\n",
    "plt.subplot(1, 2, 2)  # 1 row, 2 columns, second subplot\n",
    "plt.imshow(spike_counts_second_300, aspect='auto', cmap='viridis', interpolation='nearest')\n",
    "plt.colorbar(label='Spike Count')\n",
    "plt.title('Second 300 Bins')\n",
    "plt.xlabel('Time (bins)')\n",
    "plt.ylabel('Units')\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "plt.savefig(base_folder/'spike_raster_PC.pdf', format='pdf', dpi=600)\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c64fa26-3cdd-4e29-973a-f842c1c70359",
   "metadata": {},
   "outputs": [],
   "source": [
    "spike_counts3 = np.zeros((len(keep_unit_ids3), num_bins), dtype=int)\n",
    "unit_id_to_location = {unit_id: unit_locations[i] for i, unit_id in enumerate(keep_unit_ids3)}\n",
    "\n",
    "sorted_unit_ids = sorted(keep_unit_ids3, key=lambda unit_id: unit_id_to_location[unit_id][1])\n",
    "\n",
    "spike_counts = np.zeros((len(sorted_unit_ids), num_bins), dtype=int)\n",
    "differences = []\n",
    "for unit_idx, unit_id in enumerate(sorted_unit_ids):\n",
    "    spike_train = spike_trains_WI[unit_id]\n",
    "    binned_spikes, _ = np.histogram(spike_train, bins=num_bins, range=(0, num_samples))\n",
    "    spike_counts3[unit_idx, :] = binned_spikes\n",
    "normalized_spike_counts3 = spike_counts3 / spike_counts3.max(axis=1, keepdims=True)\n",
    "spike_counts_CA1_df = pd.DataFrame(spike_counts3)\n",
    "spike_counts_CA1_df.insert(0, 'Unit_ID', sorted_unit_ids) \n",
    "spike_counts_CA1_df.to_csv(base_folder / \"spike_count_WI_2.csv\", index=False)\n",
    "\n",
    "plt.figure(figsize=(4, 6))\n",
    "plt.imshow(spike_counts3, aspect='auto', cmap='viridis', interpolation='nearest')\n",
    "#plt.xlim(0,60)\n",
    "plt.colorbar(label='Spike Count')\n",
    "plt.xlabel('Time (bins)')\n",
    "plt.ylabel('Units')\n",
    "plt.tight_layout()\n",
    "#plt.savefig(base_folder/'spike_raster_WI.png', format='png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77dbc83-b9bd-4ec4-b553-bf465b0888b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "spike_counts_first_300 = normalized_spike_counts3[:, :300]\n",
    "spike_counts_second_300 = normalized_spike_counts3[:, 300:600]\n",
    "\n",
    "# Plotting the heatmaps as subplots\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# First 300 bins\n",
    "plt.subplot(1, 2, 1)  # 1 row, 2 columns, first subplot\n",
    "plt.imshow(spike_counts_first_300, aspect='auto', cmap='viridis', interpolation='nearest')\n",
    "plt.colorbar(label='Spike Count')\n",
    "plt.title('First 300 Bins')\n",
    "plt.xlabel('Time (bins)')\n",
    "plt.ylabel('Units')\n",
    "\n",
    "# Second 300 bins\n",
    "plt.subplot(1, 2, 2)  # 1 row, 2 columns, second subplot\n",
    "plt.imshow(spike_counts_second_300, aspect='auto', cmap='viridis', interpolation='nearest')\n",
    "plt.colorbar(label='Spike Count')\n",
    "plt.title('Second 300 Bins')\n",
    "plt.xlabel('Time (bins)')\n",
    "plt.ylabel('Units')\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "plt.savefig(base_folder/'spike_raster_WI.pdf', format='pdf', dpi=600)\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769f8fa4-4287-471d-bf9f-8007876708de",
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_id_to_location = {unit_id: unit_locations[i] for i, unit_id in enumerate(keep_unit_ids1)}\n",
    "sorted_unit_ids = sorted(keep_unit_ids1, key=lambda unit_id: unit_id_to_location[unit_id][1])\n",
    "filtered_unit_ids1 = [unit_id for unit_id in sorted_unit_ids if unit_id_to_location[unit_id][1] < 4000]\n",
    "filtered_unit_ids2 = [unit_id for unit_id in sorted_unit_ids if unit_id_to_location[unit_id][1] > 4000]\n",
    "\n",
    "#spike_counts = np.zeros((len(filtered_unit_ids1) + len(filtered_unit_ids2), num_bins), dtype=int)\n",
    "#DG_differences = []\n",
    "#CA1_differences = []\n",
    "spike_counts_CA1 = np.zeros((len(filtered_unit_ids1), num_bins), dtype=int)\n",
    "spike_counts_Cortex = np.zeros((len(filtered_unit_ids2), num_bins), dtype=int)\n",
    "\n",
    "for unit_idx, unit_id in enumerate(filtered_unit_ids1):\n",
    "    spike_train = spike_trains_narrow[unit_id]\n",
    "    binned_spikes, _ = np.histogram(spike_train, bins=num_bins, range=(0, num_samples))\n",
    "    spike_counts_CA1[unit_idx, :] = binned_spikes\n",
    "#    first_300_sum = np.sum(binned_spikes[:300])\n",
    "#    remaining_sum = np.sum(binned_spikes[300:])\n",
    "#    DG_differences.append(first_300_sum - remaining_sum)\n",
    "for unit_idx, unit_id in enumerate(filtered_unit_ids2):\n",
    "    spike_train = spike_trains_narrow[unit_id]\n",
    "    binned_spikes, _ = np.histogram(spike_train, bins=num_bins, range=(0, num_samples))\n",
    "    spike_counts_Cortex[unit_idx, :] = binned_spikes\n",
    "#    first_300_sum = np.sum(binned_spikes[:300])\n",
    "#    remaining_sum = np.sum(binned_spikes[300:])\n",
    "#    CA1_differences.append(first_300_sum - remaining_sum)\n",
    "spike_counts_CA1_df = pd.DataFrame(spike_counts_CA1)\n",
    "spike_counts_CA1_df.insert(0, 'Mapped_Unit_ID', filtered_unit_ids1)  # Insert the unit IDs as the first column\n",
    "\n",
    "spike_counts_Cortex_df = pd.DataFrame(spike_counts_Cortex)\n",
    "spike_counts_Cortex_df.insert(0, 'Mapped_Unit_ID', filtered_unit_ids2)  # Insert the unit IDs as the first column\n",
    "\n",
    "# Save the DataFrames to CSV files\n",
    "spike_counts_CA1_df.to_csv(base_folder / \"spike_count_NI_DG.csv\", index=False)\n",
    "spike_counts_Cortex_df.to_csv(base_folder / \"spike_count_NI_CA1.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd6941f-2110-4a3d-9f0b-51c910037814",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spike_counts_CA1_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c9fa2b-c1d9-4cb4-87ab-979f8a540fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_id_to_location = {unit_id: unit_locations[i] for i, unit_id in enumerate(keep_unit_ids2)}\n",
    "sorted_unit_ids2 = sorted(keep_unit_ids2, key=lambda unit_id: unit_id_to_location[unit_id][1])\n",
    "filtered_unit_ids1 = [unit_id for unit_id in sorted_unit_ids2 if unit_id_to_location[unit_id][1] < 4000]\n",
    "filtered_unit_ids2 = [unit_id for unit_id in sorted_unit_ids2 if unit_id_to_location[unit_id][1] > 4000]\n",
    "\n",
    "spike_counts_CA1 = np.zeros((len(filtered_unit_ids1), num_bins), dtype=int)\n",
    "spike_counts_Cortex = np.zeros((len(filtered_unit_ids2), num_bins), dtype=int)\n",
    "\n",
    "for unit_idx, unit_id in enumerate(filtered_unit_ids1):\n",
    "    spike_train = spike_trains_pyramidal[unit_id]\n",
    "    binned_spikes, _ = np.histogram(spike_train, bins=num_bins, range=(0, num_samples))\n",
    "    spike_counts_CA1[unit_idx, :] = binned_spikes\n",
    "#    first_300_sum = np.sum(binned_spikes[:300])\n",
    "#    remaining_sum = np.sum(binned_spikes[300:])\n",
    "#    DG_differences.append(first_300_sum - remaining_sum)\n",
    "for unit_idx, unit_id in enumerate(filtered_unit_ids2):\n",
    "    spike_train = spike_trains_pyramidal[unit_id]\n",
    "    binned_spikes, _ = np.histogram(spike_train, bins=num_bins, range=(0, num_samples))\n",
    "    spike_counts_Cortex[unit_idx, :] = binned_spikes\n",
    "#    first_300_sum = np.sum(binned_spikes[:300])\n",
    "#    remaining_sum = np.sum(binned_spikes[300:])\n",
    "#    CA1_differences.append(first_300_sum - remaining_sum)\n",
    "\n",
    "spike_counts_CA1_df = pd.DataFrame(spike_counts_CA1)\n",
    "spike_counts_CA1_df.insert(0, 'Mapped_Unit_ID', filtered_unit_ids1)  # Insert the unit IDs as the first column\n",
    "\n",
    "spike_counts_Cortex_df = pd.DataFrame(spike_counts_Cortex)\n",
    "spike_counts_Cortex_df.insert(0, 'Mapped_Unit_ID', filtered_unit_ids2)  # Insert the unit IDs as the first column\n",
    "\n",
    "# Save the DataFrames to CSV files\n",
    "spike_counts_CA1_df.to_csv(base_folder / \"spike_count_PC_DG.csv\", index=False)\n",
    "spike_counts_Cortex_df.to_csv(base_folder / \"spike_count_PC_CA1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057dba0f-5c27-4973-9494-a445c3ddb1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_id_to_location = {unit_id: unit_locations[i] for i, unit_id in enumerate(keep_unit_ids3)}\n",
    "sorted_unit_ids3 = sorted(keep_unit_ids3, key=lambda unit_id: unit_id_to_location[unit_id][1])\n",
    "filtered_unit_ids1 = [unit_id for unit_id in sorted_unit_ids3 if unit_id_to_location[unit_id][1] < 4000]\n",
    "filtered_unit_ids2 = [unit_id for unit_id in sorted_unit_ids3 if unit_id_to_location[unit_id][1] > 4000]\n",
    "\n",
    "spike_counts_CA1 = np.zeros((len(filtered_unit_ids1), num_bins), dtype=int)\n",
    "spike_counts_Cortex = np.zeros((len(filtered_unit_ids2), num_bins), dtype=int)\n",
    "\n",
    "for unit_idx, unit_id in enumerate(filtered_unit_ids1):\n",
    "    spike_train = spike_trains_WI[unit_id]\n",
    "    binned_spikes, _ = np.histogram(spike_train, bins=num_bins, range=(0, num_samples))\n",
    "    spike_counts_CA1[unit_idx, :] = binned_spikes\n",
    "#    first_300_sum = np.sum(binned_spikes[:300])\n",
    "#    remaining_sum = np.sum(binned_spikes[300:])\n",
    "#    DG_differences.append(first_300_sum - remaining_sum)\n",
    "for unit_idx, unit_id in enumerate(filtered_unit_ids2):\n",
    "    spike_train = spike_trains_WI[unit_id]\n",
    "    binned_spikes, _ = np.histogram(spike_train, bins=num_bins, range=(0, num_samples))\n",
    "    spike_counts_Cortex[unit_idx, :] = binned_spikes\n",
    "#    first_300_sum = np.sum(binned_spikes[:300])\n",
    "#    remaining_sum = np.sum(binned_spikes[300:])\n",
    "#    CA1_differences.append(first_300_sum - remaining_sum)\n",
    "spike_counts_CA1_df = pd.DataFrame(spike_counts_CA1)\n",
    "spike_counts_CA1_df.insert(0, 'Mapped_Unit_ID', filtered_unit_ids1)  # Insert the unit IDs as the first column\n",
    "\n",
    "spike_counts_Cortex_df = pd.DataFrame(spike_counts_Cortex)\n",
    "spike_counts_Cortex_df.insert(0, 'Mapped_Unit_ID', filtered_unit_ids2)  # Insert the unit IDs as the first column\n",
    "\n",
    "# Save the DataFrames to CSV files\n",
    "spike_counts_CA1_df.to_csv(base_folder / \"spike_count_WI_DG.csv\", index=False)\n",
    "spike_counts_Cortex_df.to_csv(base_folder / \"spike_count_WI_CA1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3437f24-4b51-4adc-9536-bb8cb49a1d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spikeinterface.widgets as sw\n",
    "sw.plot_sorting_summary(analyzer_WI, backend=\"spikeinterface_gui\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a02f28-4ce2-4fee-94f2-16337be6ad68",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, len(unit_ids) * 3))\n",
    "for i, unit_id in enumerate(unit_ids):\n",
    "    plt.subplot(len(unit_ids), 1, i + 1)\n",
    "    plt.hist(amplitudes[unit_id], bins=50, color='blue', edgecolor='black', alpha=0.7)\n",
    "    plt.title(f'Unit {unit_id} - Peak Spike Amplitudes')\n",
    "    plt.xlabel('Amplitude')\n",
    "    plt.ylabel('Frequency')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca02065-dc05-4c4e-a3f2-cfa18a6a5e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spikeinterface.widgets as sw\n",
    "unit_ids = sorting_SC.unit_ids[10:20]\n",
    "\n",
    "sw.plot_unit_waveforms(sorting_analyzer, unit_ids=unit_ids, figsize=(12, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20455f1d-02d7-43be-a014-2456d3e6301d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sw.plot_unit_locations(sorting_analyzer, unit_ids=unit_ids, figsize=(4, 8))\n",
    "ax.set_ylim(-100, 3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f6daa4-ebae-431a-b220-8892f7934823",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import umap\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded8bb3f-fb15-4dba-ac90-5e32ec7e669f",
   "metadata": {},
   "outputs": [],
   "source": [
    "we = si.extract_waveforms(rec, sorting_SC, folder=base_folder/'waveforms')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476b5013-3d7e-44a9-8560-bb46cdbd8697",
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assuming keep_unit_ids1, keep_unit_ids2, keep_unit_ids3, and waveform extraction functions are defined\n",
    "\n",
    "waveform_features = []\n",
    "labels = []\n",
    "\n",
    "# Iterate over all selected unit IDs and collect data\n",
    "for unit_id in keep_unit_ids1 + keep_unit_ids2 + keep_unit_ids3:\n",
    "    # Get waveforms for the unit and average them across spikes\n",
    "    waveforms = we.get_waveforms(unit_id)\n",
    "    avg_waveform = np.mean(waveforms, axis=0)  # Average across spikes\n",
    "    waveform_features.append(avg_waveform.flatten())  # Flatten and store\n",
    "    \n",
    "    # Label based on the unit group\n",
    "    if unit_id in keep_unit_ids1:\n",
    "        labels.append('NI')\n",
    "    elif unit_id in keep_unit_ids2:\n",
    "        labels.append('PC')\n",
    "    else:\n",
    "        labels.append('WI')\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "waveform_features = np.array(waveform_features)\n",
    "\n",
    "# Normalize the waveform features\n",
    "scaler = StandardScaler()\n",
    "waveform_features = scaler.fit_transform(waveform_features)\n",
    "\n",
    "# Perform UMAP on the normalized waveform features\n",
    "umap_reducer = umap.UMAP(n_components=5, random_state=42)\n",
    "waveform_umap = umap_reducer.fit_transform(waveform_features)\n",
    "\n",
    "# Convert labels to numpy array\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Create a figure for plotting\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "unique_labels = np.unique(labels)\n",
    "colors = ['blue', 'green', 'red']\n",
    "\n",
    "# Plot the UMAP results\n",
    "for i, label in enumerate(unique_labels):\n",
    "    indices = np.where(labels == label)\n",
    "    ax.scatter(waveform_umap[indices, 0], waveform_umap[indices, 1], \n",
    "               c=colors[i], label=label, alpha=0.5)\n",
    "\n",
    "ax.set_xlabel('UMAP 1')\n",
    "ax.set_ylabel('UMAP 2')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47be2b3-d852-403b-bbc5-dd0510ba399f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Perform LDA on the waveform features\n",
    "lda = LDA(n_components=2)\n",
    "waveform_lda = lda.fit_transform(waveform_features, labels)\n",
    "\n",
    "# Create an LDA plot\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "unique_labels = np.unique(labels)\n",
    "colors = ['blue', 'green', 'red']\n",
    "\n",
    "for i, label in enumerate(unique_labels):\n",
    "    indices = np.where(labels == label)\n",
    "    ax.scatter(waveform_lda[indices, 0], waveform_lda[indices, 1], \n",
    "               c=colors[i], label=label, alpha=0.5)\n",
    "\n",
    "ax.set_xlabel('LDA 1')\n",
    "ax.set_ylabel('LDA 2')\n",
    "ax.legend()\n",
    "plt.title('LDA of Neuron Waveforms')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e401a6-01a6-40f0-a7b1-77bf0ed974a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_metrics=template_metrics[\"peak_to_valley\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384b0f02-109d-47e2-a976-e5bd02d423d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_wide, y_wide = [], []\n",
    "x_narrow, y_narrow = [], []\n",
    "x_pyramidal, y_pyramidal = [], []\n",
    "\n",
    "# Collect data for wide interneurons\n",
    "for unit_id in keep_unit_ids1:\n",
    "    x_wide.append(template_metrics[unit_id])\n",
    "    y_wide.append(mapped_tau_rise[unit_id])\n",
    "\n",
    "# Collect data for narrow interneurons\n",
    "for unit_id in keep_unit_ids2:\n",
    "    x_narrow.append(template_metrics[unit_id])\n",
    "    y_narrow.append(mapped_tau_rise[unit_id])\n",
    "\n",
    "# Collect data for pyramidal cells\n",
    "for unit_id in keep_unit_ids3:\n",
    "    x_pyramidal.append(template_metrics[unit_id])\n",
    "    y_pyramidal.append(mapped_tau_rise[unit_id])\n",
    "\n",
    "colors = ['blue', 'green', 'red']\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "# Plot Narrow Interneuron (NI)\n",
    "plt.scatter(x_wide, y_wide, edgecolor=colors[0], facecolor=colors[0], label='NI', marker='o', alpha=0.5, linewidths=1.5)\n",
    "\n",
    "# Plot Pyramidal Cell (PC)\n",
    "plt.scatter(x_narrow, y_narrow, edgecolor=colors[1], facecolor=colors[1], label='PC', marker='o', alpha=0.5, linewidths=1.5)\n",
    "\n",
    "# Plot Wide Interneuron (WI)\n",
    "plt.scatter(x_pyramidal, y_pyramidal, edgecolor=colors[2], facecolor=colors[2], label='WI', marker='o', alpha=0.5, linewidths=1.5)\n",
    "\n",
    "plt.xlabel('Template Metric (e.g., Peak-to-Valley)')\n",
    "plt.ylabel('Tau Rise')\n",
    "plt.title('Neurons by Template Metric and Tau Rise')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c668abeb-43e9-4d29-ad5a-e7efe3f9a5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm\n",
    "probe = rec.get_probe()\n",
    "channel_locations = probe.contact_positions  # Get the electrode positions\n",
    "filtered_indices = np.isin(unit_ids, keep_unit_ids1)\n",
    "filtered_x_coords = unit_locations[filtered_indices, 0]\n",
    "filtered_y_coords = unit_locations[filtered_indices, 1]\n",
    "\n",
    "# Generate a color map with a unique color for each unit\n",
    "colors = cm.rainbow(np.linspace(0, 1, len(keep_unit_ids1)))\n",
    "\n",
    "# Plot the Neuropixels probe layout\n",
    "plt.figure(figsize=(8, 10))\n",
    "plt.scatter(channel_locations[:, 0], channel_locations[:, 1], c='gray', marker='o', label='Electrodes', alpha=0.5)\n",
    "\n",
    "for i, unit_id in enumerate(keep_unit_ids1):\n",
    "    unit_x = filtered_x_coords[unit_ids[filtered_indices] == unit_id]\n",
    "    unit_y = filtered_y_coords[unit_ids[filtered_indices] == unit_id]\n",
    "    plt.scatter(unit_x, unit_y, c=[colors[i]], marker='o', label=f'Unit {unit_id}')\n",
    "\n",
    "plt.title('Pyramidal Cell Locations on Neuropixels Probe')\n",
    "plt.xlabel('X Coordinate')\n",
    "plt.ylabel('Y Coordinate')\n",
    "#plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555ce305-483a-4ee3-b0d2-d0ba28e1bb5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
